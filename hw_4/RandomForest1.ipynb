{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hit some part of sky - subtract away static background and things that remain are interesting\n",
    "\n",
    "To get static take many images and add on top of each other and take the median\n",
    "\n",
    "But subtracting stuff generates artifacts - misalignment can be a big cause - or cosmic rays - satellites can go overhead and leave trails - edge of the telescope can have senstitivities and distortions - and really bright stars can saturate the detector\n",
    "\n",
    "portal.nersc.gov/project/dessn.autoscan/\n",
    "\n",
    "iopscience.iop.org/article/10.1088/0004-6256/150/3/82/pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define functions that take in some data and compute features - \n",
    "# output is numerical features \n",
    "diffsumrn\n",
    "    imgrn is an array\n",
    "    the diff key returns the subtracted image in numpy form\n",
    "    renormalized the image to make it comparable \n",
    "    think of it as the sum of the differences\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Astromatic - source extractor = can use to measure brightness in image - how bright someting = SExtractor- based on factor between\n",
    "source extraction and photometry - github kbarbary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn - random forest - hyperparameters\n",
    "scikit-learn random forest documentation\n",
    "youtube - Daniel Goldstein - how autoscan works\n",
    "1st draw with replacement a subset of data and put at the root of the tree - then grow the tree recursively until you reach a stopping criterion - have 38 features and want to make a cut on one of the features to make a separation - find the cut on the features that separates the data best into its constituent classes - and that's a hyperparameter - and you need to do a search over that to find the best value\n",
    "Lots of ways to define the best split - information gain - gini coefficient - \"information criterion for random forest root split\" as google term\n",
    "The number of trees you grow is a parameter\n",
    "All the trees vote and take a weighted average of all the predictions of each tree (the weight is the prob that the tree assigns)\n",
    "When thresholding, there's a tradeoff between purity and efficiency\n",
    "FPR = false positive rate\n",
    "MDR - missed detection rate\n",
    "n_esitmators = number of trees to grow\n",
    "number of samples you need in a node to split it (i.e. where tree stops growing\n",
    "max_depth = maximum depth that ca\n",
    "sklearn grid search \n",
    "    exchaustive grid search - GridSearchCV\n",
    "        scoring - returns val of model fgiven your hyperparameters\n",
    "        cv - cross-validation - good because gives you a sense of how well your model generalizeds - for every hyperparameter you're trying\n",
    "    randomized p\n",
    "PCA on images - principal component analysis on your images\n",
    "Eigenfaces - use the coefficients of each eigenface as an imput into the random forest\n",
    "youtube - PCA 10:eigen-faces\n",
    "PCA gives you like 10 eigenvectors - can reconstruct original images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
