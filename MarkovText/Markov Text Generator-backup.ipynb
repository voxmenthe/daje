{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda2_40\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# create a text file with current articles from politico:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "polrequest = requests.get('http://www.politico.com/')\n",
    "politico_soup = BeautifulSoup(polrequest.content)\n",
    "\n",
    "# create an array of story links from the home page\n",
    "def generateLinks(soupobject):\n",
    "    stories = []\n",
    "    for link in soupobject.findAll('a'):\n",
    "        try:\n",
    "            if 'www.politico.com/story/' in link.attrs['href']:\n",
    "                if link.attrs['href'] not in stories:\n",
    "                    stories.append(link.attrs['href'])\n",
    "        \n",
    "        except KeyError:\n",
    "            continue\n",
    "    # save links as file\n",
    "    with open('politicostorylinks.txt','w') as export:\n",
    "        delim = '\\n'\n",
    "        export.write(delim.join(stories))\n",
    "        export.close()\n",
    "    #stories = list(set(stories))\n",
    "    return stories\n",
    "\n",
    "def generateNonOverlapLinks(soupobject):\n",
    "    newstories = []\n",
    "    plinks = open('politicostorylinks.txt','r')\n",
    "    for link in soupobject.findAll('a'):\n",
    "        try:\n",
    "            if 'www.politico.com/story/' in link.attrs['href']:\n",
    "                if link.attrs['href'] not in plinks:\n",
    "                    newstories.append(link.attrs['href'])\n",
    "        \n",
    "        except KeyError:\n",
    "            continue\n",
    "    plinks.close()\n",
    "    return newstories\n",
    "    \n",
    "    \n",
    "# now send http requests to get the stories\n",
    "#set(stories)\n",
    "\n",
    "def get_politico_story(html):\n",
    "    result = \"\"\n",
    "    soup = BeautifulSoup(html)\n",
    "    story_text = soup.find('div', class_='story-text')\n",
    "    if story_text is None:\n",
    "        raise ValueError('html did not contain a story.')\n",
    "    paragraphs = story_text.findAll('p', class_=None)\n",
    "    for paragraph in paragraphs:\n",
    "        result += paragraph.getText() + '\\n\\n'\n",
    "    return result\n",
    "\n",
    "def get_all_politico_text(urlarray):\n",
    "    hugetextfile = \"\"\n",
    "    for item in urlarray:\n",
    "        request = requests.get(item)\n",
    "        textholder = request.content\n",
    "        textholder = get_politico_story(textholder)\n",
    "        hugetextfile = hugetextfile + \" \" + textholder\n",
    "    return hugetextfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all the text from the home page articles\n",
    "#politicotextdump = get_all_politico_text(generateLinks(politico_soup))\n",
    "politicotextdump = get_all_politico_text(generateNonOverlapLinks(politico_soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean the text\n",
    "def cleanText(textdump):\n",
    "    import string\n",
    "    \n",
    "    # remove non-ascii characters\n",
    "    textdump = ''.join([i if ord(i) < 128 else ' ' for i in textdump])\n",
    "    \n",
    "    # clean some other stuff\n",
    "    exclude = set([\"\\n\",\"Getty\",\"|\",\"”\",\"“\",\"]\",\"[\",\"AP Photo\"])\n",
    "    textdump = ''.join(item for item in textdump if item not in exclude)\n",
    "    \n",
    "    # add space after periods\n",
    "    textdump = string.replace(textdump, '.', '. ')\n",
    "    textdump = string.replace(textdump, 'Getty', '')\n",
    "    textdump = string.replace(textdump, '/POLITICO', '')\n",
    "    textdump = string.replace(textdump, 'AP Photo', '')\n",
    "    \n",
    "    return textdump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\" people under the bus.  I know there s a lot of controversy on the Hispanic vote.  People are always saying,  Oh, you can t win if you don't have the Hispanic vote.   He pauses to adjust the glasses and reflects for the briefest of moments on his own political twilight, and a potentially tough 2016 reelection campaign.  If there's anybody that should be shying away from Trump, it s me, with all the heat I m taking, right?  he said.   Any smart guy would do that, but I m not a smart guy, maybe. \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to think about what to do with punctuation\n",
    "# cases for sentence ends and beginnings (capitalization and periods)\n",
    "# and possibly for text within quotes\n",
    "politicocleaned = cleanText(politicotextdump)\n",
    "poltxtarr = politicocleaned.split(\" \")\n",
    "politicocleaned[-500:-1]\n",
    "#poltxtarr[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# append text to file\n",
    "with open(\"politicocleaned.txt\", \"a\") as export:\n",
    "    export.write(politicocleaned)\n",
    "    export.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump text blob to file\n",
    "with open('politicocleaned.txt','w') as export:\n",
    "    export.write(politicocleaned)\n",
    "    export.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load saved text from file\n",
    "with open(\"politicocleaned.txt\", \"r\") as imported:\n",
    "    allpoliticotext = imported.read()\n",
    "    imported.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397892"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#allpoliticotext[-500:-1]\n",
    "len(allpoliticotext.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getNgrams(longstring,ngramlen):\n",
    "    import numpy as np\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    document = [longstring]\n",
    "    vectorizer = CountVectorizer(ngram_range=(ngramlen, ngramlen))\n",
    "\n",
    "    # Don't need both X and transformer; they should be identical\n",
    "    X = vectorizer.fit_transform(document)\n",
    "    matrix_terms = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "    # Use the axis keyword to sum over rows\n",
    "    matrix_freq = np.asarray(X.sum(axis=0)).ravel()\n",
    "    final_matrix = np.array([matrix_terms,matrix_freq])\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    freqs = X.sum(axis=0).A1\n",
    "    result = dict(zip(terms, freqs))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf8' codec can't decode byte 0x92 in position 538: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-db9ec7f354f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#allpoliticotext = cleanText(allpoliticotext)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfreqdict1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetNgrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallpoliticotext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfreqdict2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetNgrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallpoliticotext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfreqdict3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetNgrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallpoliticotext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfreqdict4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetNgrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallpoliticotext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-a157e389d767>\u001b[0m in \u001b[0;36mgetNgrams\u001b[1;34m(longstring, ngramlen)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Don't need both X and transformer; they should be identical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mmatrix_terms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2_40\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 817\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2_40\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2_40\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 238\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2_40\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda2_40\\lib\\encodings\\utf_8.pyc\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(input, errors)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf8' codec can't decode byte 0x92 in position 538: invalid start byte"
     ]
    }
   ],
   "source": [
    "#allpoliticotext = cleanText(allpoliticotext)\n",
    "freqdict1 = getNgrams(allpoliticotext,1)\n",
    "freqdict2 = getNgrams(allpoliticotext,2)\n",
    "freqdict3 = getNgrams(allpoliticotext,3)\n",
    "freqdict4 = getNgrams(allpoliticotext,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get distribution of follow-on words for ngrams in smallNdict\n",
    "def getNgramDist(smallNdict,bigNdict):\n",
    "    output = {}\n",
    "    for key in smallNdict:\n",
    "        output[key] = []\n",
    "    for key, value in smallNdict.items():\n",
    "        for key2, value2, in bigNdict.items():\n",
    "            small = key.split(\" \")\n",
    "            big = key2.split(\" \")\n",
    "            if set(small).issubset(set(big)) and small[-1] == big[-2]:\n",
    "               output[key].append([big[-1],value2])\n",
    "    return output                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNgramDist2(text,n):\n",
    "    # return {ngram: {followon word: counts), ...} from corpus text.\n",
    "    result = {}\n",
    "    spltext = text.split()\n",
    "    if n < 1:\n",
    "        raiseValueError('n must be greater than 1')\n",
    "    for i in range(len(spltext)):\n",
    "        if i + n + 1 < len(spltext):\n",
    "            words = spltext[i:i+n+1]\n",
    "            ngram = ' '.join(words[:-1])\n",
    "            followon = words[-1]\n",
    "            if ngram in result:\n",
    "                if followon in result[ngram]:\n",
    "                    result[ngram][followon] += 1\n",
    "                else:\n",
    "                    result[ngram][followon] = 1\n",
    "            else:\n",
    "                result[ngram] = {}\n",
    "                result[ngram][followon] = 1                    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allpoliticotext = cleanText(allpoliticotext)\n",
    "Ngram1Dist = getNgramDist2(allpoliticotext,1)\n",
    "Ngram2Dist = getNgramDist2(allpoliticotext,2)\n",
    "Ngram3Dist = getNgramDist2(allpoliticotext,3)\n",
    "Ngram4Dist = getNgramDist2(allpoliticotext,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Ngram2Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes a long time to calculate - execute with caution\n",
    "#Ngram1Dist = getNgramDist(freqdict1,freqdict2)\n",
    "# takes a long time to calculate - execute with caution\n",
    "#Ngram2Dist = getNgramDist(freqdict2,freqdict3)\n",
    "# takes a long time to calculate - execute with caution\n",
    "#Ngram3Dist = getNgramDist(freqdict3,freqdict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save ngramDicts as json\n",
    "def jsondump(filename,data):\n",
    "    import json\n",
    "    with open(filename,'w') as outfile:\n",
    "        json.dump(str(data),outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsondump('ngram1Dist.txt',Ngram1Dist)\n",
    "jsondump('ngram2Dist.txt',Ngram2Dist)\n",
    "jsondump('ngram3Dist.txt',Ngram3Dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load json data:\n",
    "import json\n",
    "ngDist1 = eval(json.loads(open('ngram1Dist.txt').read()))\n",
    "ngDist2 = eval(json.loads(open('ngram2Dist.txt').read()))\n",
    "ngDist3 = eval(json.loads(open('ngram3Dist.txt').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gen string of 1 grams\n",
    "def markov_1grams(nwords):\n",
    "    # Generate a Markov chain of text from the 1 grams trans\n",
    "    # matrix\n",
    "    import numpy as np\n",
    "    result = \"\"\n",
    "    if nwords < 1:\n",
    "        raise ValueError('nwords must be greater than 1.')\n",
    "    else:\n",
    "        result = random.choice(ngDist1.keys())\n",
    "        nwords-= 1\n",
    "        for i in range(nwords):\n",
    "            key = result.split()[-1]\n",
    "            ngDist1[key]\n",
    "            words = zip(*ngDist1[key])[0]\n",
    "            probs = np.array(zip(*ngDist1[key])[1],dtype=np.float32)\n",
    "            probs /= probs.sum()\n",
    "            newword = np.random.choice(words,p=probs)\n",
    "            result += ' ' + newword\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate string of markov text taking distribution\n",
    "# dictionary and length of output text in words\n",
    "# as inputs\n",
    "def markov_text(dictionary, nwords):\n",
    "    import numpy as np\n",
    "    result = \"\"\n",
    "    if nwords < 1:\n",
    "        raise ValueError('nwords must be greater than 1.')\n",
    "    else:\n",
    "        result = random.choice(dictionary.keys())\n",
    "        nwords -= 1\n",
    "        for i in range(nwords):\n",
    "            key = result.split()[-1]\n",
    "            dictionary[key]\n",
    "            words = zip(*dictionary[key])[0]\n",
    "            \n",
    "            # put the probabilities in the transition matrix\n",
    "            probs = np.array(zip(*dictionary[key])[1],dtype=np.float32)\n",
    "            probs /= probs.sum()\n",
    "\n",
    "            # choose the next word based on the transition matrix\n",
    "            newword = np.random.choice(words,p=probs)\n",
    "\n",
    "            result += ' ' + newword\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: i",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-04e469993b7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmarkov_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNgram1Dist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-51-7af973da4e96>\u001b[0m in \u001b[0;36mmarkov_text\u001b[1;34m(dictionary, nwords)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;31m# put the probabilities in the transition matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: i"
     ]
    }
   ],
   "source": [
    "markov_text(Ngram1Dist,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "words = politicotextdump.split(\" \")\n",
    "counts = Counter(words).most_common(len(words))\n",
    "countsunzipped = zip(*counts)\n",
    "#counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the text file in\n",
    "def readText(filename):\n",
    "    with open(filename, 'r') as corpus:\n",
    "        data=corpus.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
