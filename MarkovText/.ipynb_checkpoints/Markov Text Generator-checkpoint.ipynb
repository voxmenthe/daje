{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeff\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# create a text file with current articles from politico:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "polrequest = requests.get('http://www.politico.com/')\n",
    "politico_soup = BeautifulSoup(polrequest.content)\n",
    "\n",
    "# create an array of story links from the home page\n",
    "def generateLinks(soupobject):\n",
    "    stories = []\n",
    "    for link in soupobject.findAll('a'):\n",
    "        try:\n",
    "            if 'www.politico.com/story/' in link.attrs['href']:\n",
    "                if link.attrs['href'] not in stories:\n",
    "                    stories.append(link.attrs['href'])\n",
    "        \n",
    "        except KeyError:\n",
    "            continue\n",
    "    # save links as file\n",
    "    with open('politicostorylinks.txt','w') as export:\n",
    "        delim = '\\n'\n",
    "        export.write(delim.join(stories))\n",
    "        export.close()\n",
    "    #stories = list(set(stories))\n",
    "    return stories\n",
    "\n",
    "def generateNonOverlapLinks(soupobject):\n",
    "    newstories = []\n",
    "    plinks = open('politicostorylinks.txt','r')\n",
    "    for link in soupobject.findAll('a'):\n",
    "        try:\n",
    "            if 'www.politico.com/story/' in link.attrs['href']:\n",
    "                if link.attrs['href'] not in plinks:\n",
    "                    newstories.append(link.attrs['href'])\n",
    "        \n",
    "        except KeyError:\n",
    "            continue\n",
    "    plinks.close()\n",
    "    return newstories\n",
    "    \n",
    "    \n",
    "# now send http requests to get the stories\n",
    "#set(stories)\n",
    "\n",
    "def get_politico_story(html):\n",
    "    result = \"\"\n",
    "    soup = BeautifulSoup(html)\n",
    "    story_text = soup.find('div', class_='story-text')\n",
    "    if story_text is None:\n",
    "        raise ValueError('html did not contain a story.')\n",
    "    paragraphs = story_text.findAll('p', class_=None)\n",
    "    for paragraph in paragraphs:\n",
    "        result += paragraph.getText() + '\\n\\n'\n",
    "    return result\n",
    "\n",
    "def get_all_politico_text(urlarray):\n",
    "    hugetextfile = \"\"\n",
    "    for item in urlarray:\n",
    "        request = requests.get(item)\n",
    "        textholder = request.content\n",
    "        textholder = get_politico_story(textholder)\n",
    "        hugetextfile = hugetextfile + \" \" + textholder\n",
    "    return hugetextfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeff\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# get all the text from the home page articles\n",
    "politicotextdump = get_all_politico_text(generateLinks(politico_soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean the text\n",
    "def cleanText(textdump):\n",
    "    import string\n",
    "    exclude = set([\"'\",\"\\n\",\"Getty\",\"|\",'\"',\"”\",\"“\",])\n",
    "    textdump = ''.join(item for item in textdump if item not in exclude)\n",
    "    return textdump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                 People wait for the start of a memorial service on June 19, 2016, in Orlando, Florida.   GettySupport for stronger gun control measures increased by 9 percentage points in recent days, after 49 people were killed inside an LGBT nightclub in Orlando, Florida, according to the results of a new CNN/ORC poll released Monday.The poll found 55 percent of Americans are in favor of stricter gun laws, with 42 percent opposed, which matches the highest level of support since the poll was '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to think about what to do with punctuation\n",
    "# cases for sentence ends and beginnings (capitalization and periods)\n",
    "# and possibly for text within quotes\n",
    "politicocleaned = cleanText(politicotextdump)\n",
    "poltxtarr = politicocleaned.split(\" \")\n",
    "politicocleaned[0:500]\n",
    "#poltxtarr[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump text blob to file\n",
    "with open('politicocleaned.txt','w') as export:\n",
    "    export.write(politicocleaned)\n",
    "    export.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# append text to file\n",
    "with open(\"politicocleaned.txt\", \"a\") as export:\n",
    "    export.write(politicocleaned)\n",
    "    export.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load saved text from file\n",
    "with open(\"politicocleaned.txt\", \"r\") as imported:\n",
    "    allpoliticotext = imported.read()\n",
    "    imported.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getNgrams(longstring,ngramlen):\n",
    "    import numpy as np\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    document = [longstring]\n",
    "    vectorizer = CountVectorizer(ngram_range=(ngramlen, ngramlen))\n",
    "\n",
    "    # Don't need both X and transformer; they should be identical\n",
    "    X = vectorizer.fit_transform(document)\n",
    "    matrix_terms = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "    # Use the axis keyword to sum over rows\n",
    "    matrix_freq = np.asarray(X.sum(axis=0)).ravel()\n",
    "    final_matrix = np.array([matrix_terms,matrix_freq])\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    freqs = X.sum(axis=0).A1\n",
    "    result = dict(zip(terms, freqs))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freqdict1 = getNgrams(allpoliticotext,1)\n",
    "freqdict2 = getNgrams(allpoliticotext,2)\n",
    "freqdict3 = getNgrams(allpoliticotext,3)\n",
    "freqdict4 = getNgrams(allpoliticotext,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare12 = {}\n",
    "for key in freqdict1:\n",
    "    compare12[key] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare23 = {}\n",
    "for key in freqdict2:\n",
    "    compare23[key] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compare23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get distribution of follow-on words for ngrams in smallNdict\n",
    "def getNgramDist(smallNdict,bigNdict):\n",
    "    output = {}\n",
    "    for key in smallNdict:\n",
    "        output[key] = []\n",
    "    for key, value in smallNdict.items():\n",
    "        for key2, value2, in bigNdict.items():\n",
    "            small = key.split(\" \")\n",
    "            big = key2.split(\" \")\n",
    "            if set(small).issubset(set(big)) and small[-1] == big[-2]:\n",
    "               output[key].append([big[-1],value2])\n",
    "    return output                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes a long time to calculate - execute with caution\n",
    "Ngram1Dist = getNgramDist(freqdict1,freqdict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# takes a long time to calculate - execute with caution\n",
    "Ngram2Dist = getNgramDist(freqdict2,freqdict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes a long time to calculate - execute with caution\n",
    "Ngram3Dist = getNgramDist(freqdict3,freqdict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save ngramDicts as json\n",
    "def jsondump(filename,data):\n",
    "    import json\n",
    "    with open(filename,'w') as outfile:\n",
    "        json.dump(str(data),outfile)\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsondump('ngram1Dist.txt',Ngram1Dist)\n",
    "jsondump('ngram2Dist.txt',Ngram2Dist)\n",
    "jsondump('ngram3Dist.txt',Ngram3Dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load json data:\n",
    "import json\n",
    "ngDist1 = json.loads(open('ngram1Dist.txt').read())\n",
    "ngDist2 = json.loads(open('ngram2Dist.txt').read())\n",
    "ngDist3 = json.loads(open('ngram3Dist.txt').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" [['that',\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngDist1[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "words = politicotextdump.split(\" \")\n",
    "counts = Counter(words).most_common(len(words))\n",
    "countsunzipped = zip(*counts)\n",
    "#counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the text file in\n",
    "def readText(filename):\n",
    "    with open(filename, 'r') as corpus:\n",
    "        data=corpus.read().replace('\\n', '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
